{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ac99ef9",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Setup & Imports\n",
    "\n",
    "First, we import the necessary libraries. EventFlow uses **Polars** for fast DataFrame operations and **NumPy** for array handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa1e037e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… EventFlow adapters loaded successfully!\n",
      "\n",
      "Available adapters:\n",
      "  â€¢ TableAdapter    - For regression models (GLM, XGBoost)\n",
      "  â€¢ SequenceAdapter - For sequence models (LSTM, Transformer)\n",
      "  â€¢ RasterAdapter   - For grid models (CNN, ConvLSTM)\n",
      "  â€¢ GraphAdapter    - For graph models (GCN, GAT)\n",
      "  â€¢ StreamAdapter   - For continuous-time models (Neural ODE)\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "# EventFlow imports\n",
    "import sys\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"src\"))\n",
    "\n",
    "from eventflow.core.adapters import (\n",
    "    # Each adapter has a Config class for settings and an Adapter class for conversion\n",
    "    TableAdapter, TableAdapterConfig,\n",
    "    SequenceAdapter, SequenceAdapterConfig,\n",
    "    RasterAdapter, RasterAdapterConfig,\n",
    "    GraphAdapter, GraphAdapterConfig,\n",
    "    StreamAdapter, StreamAdapterConfig,\n",
    ")\n",
    "\n",
    "print(\"âœ… EventFlow adapters loaded successfully!\")\n",
    "print(f\"\\nAvailable adapters:\")\n",
    "print(\"  â€¢ TableAdapter    - For regression models (GLM, XGBoost)\")\n",
    "print(\"  â€¢ SequenceAdapter - For sequence models (LSTM, Transformer)\")\n",
    "print(\"  â€¢ RasterAdapter   - For grid models (CNN, ConvLSTM)\")\n",
    "print(\"  â€¢ GraphAdapter    - For graph models (GCN, GAT)\")\n",
    "print(\"  â€¢ StreamAdapter   - For continuous-time models (Neural ODE)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7576e9",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“Š Step 1: Generate Sample Crime Data\n",
    "\n",
    "We'll create synthetic Chicago crime data that mimics the real dataset structure. Each record represents a single crime incident with:\n",
    "\n",
    "- **Spatial info**: Latitude, longitude, beat, district\n",
    "- **Temporal info**: Timestamp of when the crime occurred\n",
    "- **Categorical info**: Crime type (theft, battery, etc.)\n",
    "- **Binary flags**: Whether an arrest was made, domestic incident\n",
    "\n",
    "### Why Synthetic Data?\n",
    "Using synthetic data lets us:\n",
    "1. Run the demo without downloading external files\n",
    "2. Control the data characteristics for clear demonstrations\n",
    "3. Ensure reproducibility with `np.random.seed(42)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62097769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ Generated 1,000 crime records\n",
      "ğŸ“… Date range: 2024-01-01 00:00:00 to 2024-01-30 23:00:00\n",
      "ğŸ—ºï¸  Spatial bounds: (41.64, -87.94) to (42.02, -87.52)\n",
      "\n",
      "ğŸ” Sample records:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>case_id</th><th>timestamp</th><th>latitude</th><th>longitude</th><th>primary_type</th><th>beat</th><th>district</th><th>domestic</th><th>arrest</th></tr><tr><td>str</td><td>datetime[Î¼s]</td><td>f64</td><td>f64</td><td>str</td><td>i32</td><td>i32</td><td>bool</td><td>bool</td></tr></thead><tbody><tr><td>&quot;JE100000&quot;</td><td>2024-01-05 06:00:00</td><td>41.992051</td><td>-87.828537</td><td>&quot;ROBBERY&quot;</td><td>2107</td><td>17</td><td>false</td><td>false</td></tr><tr><td>&quot;JE100001&quot;</td><td>2024-01-19 03:00:00</td><td>41.914207</td><td>-87.865338</td><td>&quot;NARCOTICS&quot;</td><td>1463</td><td>10</td><td>true</td><td>true</td></tr><tr><td>&quot;JE100002&quot;</td><td>2024-01-12 06:00:00</td><td>41.658276</td><td>-87.64952</td><td>&quot;ASSAULT&quot;</td><td>2487</td><td>1</td><td>false</td><td>true</td></tr><tr><td>&quot;JE100003&quot;</td><td>2024-01-05 10:00:00</td><td>41.936976</td><td>-87.680418</td><td>&quot;BURGLARY&quot;</td><td>303</td><td>3</td><td>false</td><td>true</td></tr><tr><td>&quot;JE100004&quot;</td><td>2024-01-03 23:00:00</td><td>41.954618</td><td>-87.67406</td><td>&quot;ASSAULT&quot;</td><td>260</td><td>8</td><td>false</td><td>false</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 9)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ case_id  â”† timestamp    â”† latitude  â”† longitude  â”† â€¦ â”† beat â”† district â”† domestic â”† arrest â”‚\n",
       "â”‚ ---      â”† ---          â”† ---       â”† ---        â”†   â”† ---  â”† ---      â”† ---      â”† ---    â”‚\n",
       "â”‚ str      â”† datetime[Î¼s] â”† f64       â”† f64        â”†   â”† i32  â”† i32      â”† bool     â”† bool   â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ JE100000 â”† 2024-01-05   â”† 41.992051 â”† -87.828537 â”† â€¦ â”† 2107 â”† 17       â”† false    â”† false  â”‚\n",
       "â”‚          â”† 06:00:00     â”†           â”†            â”†   â”†      â”†          â”†          â”†        â”‚\n",
       "â”‚ JE100001 â”† 2024-01-19   â”† 41.914207 â”† -87.865338 â”† â€¦ â”† 1463 â”† 10       â”† true     â”† true   â”‚\n",
       "â”‚          â”† 03:00:00     â”†           â”†            â”†   â”†      â”†          â”†          â”†        â”‚\n",
       "â”‚ JE100002 â”† 2024-01-12   â”† 41.658276 â”† -87.64952  â”† â€¦ â”† 2487 â”† 1        â”† false    â”† true   â”‚\n",
       "â”‚          â”† 06:00:00     â”†           â”†            â”†   â”†      â”†          â”†          â”†        â”‚\n",
       "â”‚ JE100003 â”† 2024-01-05   â”† 41.936976 â”† -87.680418 â”† â€¦ â”† 303  â”† 3        â”† false    â”† true   â”‚\n",
       "â”‚          â”† 10:00:00     â”†           â”†            â”†   â”†      â”†          â”†          â”†        â”‚\n",
       "â”‚ JE100004 â”† 2024-01-03   â”† 41.954618 â”† -87.67406  â”† â€¦ â”† 260  â”† 8        â”† false    â”† false  â”‚\n",
       "â”‚          â”† 23:00:00     â”†           â”†            â”†   â”†      â”†          â”†          â”†        â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "n_records = 1000\n",
    "\n",
    "# Chicago geographic bounds (approximate)\n",
    "# These define the spatial extent of our synthetic data\n",
    "lat_min, lat_max = 41.64, 42.02  # ~42km north-south\n",
    "lon_min, lon_max = -87.94, -87.52  # ~32km east-west\n",
    "\n",
    "# Common crime types in Chicago\n",
    "crime_types = [\"THEFT\", \"BATTERY\", \"ASSAULT\", \"BURGLARY\", \"ROBBERY\", \"NARCOTICS\"]\n",
    "\n",
    "# Generate timestamps spread over 30 days\n",
    "base_date = datetime(2024, 1, 1)\n",
    "dates = [base_date + timedelta(hours=np.random.randint(0, 24*30)) for _ in range(n_records)]\n",
    "\n",
    "# Create the DataFrame\n",
    "crime_df = pl.DataFrame({\n",
    "    \"case_id\": [f\"JE{100000 + i}\" for i in range(n_records)],\n",
    "    \"timestamp\": dates,\n",
    "    \"latitude\": np.random.uniform(lat_min, lat_max, n_records),\n",
    "    \"longitude\": np.random.uniform(lon_min, lon_max, n_records),\n",
    "    \"primary_type\": np.random.choice(crime_types, n_records),\n",
    "    \"beat\": np.random.randint(100, 2500, n_records),\n",
    "    \"district\": np.random.randint(1, 26, n_records),\n",
    "    \"domestic\": np.random.choice([True, False], n_records, p=[0.15, 0.85]),\n",
    "    \"arrest\": np.random.choice([True, False], n_records, p=[0.25, 0.75]),\n",
    "})\n",
    "\n",
    "print(f\"ğŸ“‹ Generated {len(crime_df):,} crime records\")\n",
    "print(f\"ğŸ“… Date range: {crime_df['timestamp'].min()} to {crime_df['timestamp'].max()}\")\n",
    "print(f\"ğŸ—ºï¸  Spatial bounds: ({lat_min:.2f}, {lon_min:.2f}) to ({lat_max:.2f}, {lon_max:.2f})\")\n",
    "print(f\"\\nğŸ” Sample records:\")\n",
    "crime_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c10404",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ—ºï¸ Step 2: Spatial Binning & Temporal Aggregation\n",
    "\n",
    "Most ML models for crime prediction work with **aggregated counts** rather than individual incidents. We need to:\n",
    "\n",
    "1. **Divide space into grid cells** - Create a 10Ã—10 grid over Chicago\n",
    "2. **Group by time windows** - Aggregate to daily counts\n",
    "3. **Compute summary statistics** - Total events, arrests, etc.\n",
    "\n",
    "### The Grid System\n",
    "```\n",
    "â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”\n",
    "â”‚ 0 â”‚ 1 â”‚ 2 â”‚ 3 â”‚ 4 â”‚ 5 â”‚ 6 â”‚ 7 â”‚ 8 â”‚ 9 â”‚  â† lon_bin\n",
    "â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤\n",
    "â”‚10 â”‚11 â”‚12 â”‚...â”‚   â”‚   â”‚   â”‚   â”‚   â”‚19 â”‚\n",
    "â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤\n",
    "â”‚...â”‚   â”‚   â”‚   â”‚ cell_id = lat_bin*10 + lon_bin\n",
    "â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤\n",
    "â”‚90 â”‚91 â”‚92 â”‚...â”‚   â”‚   â”‚   â”‚   â”‚   â”‚99 â”‚\n",
    "â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜\n",
    "  â†‘\n",
    "lat_bin\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07f7a1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—ºï¸  Creating a 10Ã—10 spatial grid (100 cells total)\n",
      "\n",
      "ğŸ“Š Aggregation Results:\n",
      "   â€¢ Original records: 1,000\n",
      "   â€¢ Aggregated rows: 858 (cell Ã— day combinations)\n",
      "   â€¢ Unique cells with events: 100\n",
      "   â€¢ Date range: 2024-01-01 to 2024-01-30\n",
      "\n",
      "ğŸ” Sample aggregated data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>cell_id</th><th>date</th><th>lat_bin</th><th>lon_bin</th><th>event_count</th><th>arrest_count</th><th>domestic_count</th><th>centroid_lat</th><th>centroid_lon</th></tr><tr><td>i32</td><td>date</td><td>i32</td><td>i32</td><td>u32</td><td>u32</td><td>u32</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>8</td><td>2024-01-01</td><td>0</td><td>8</td><td>1</td><td>1</td><td>0</td><td>41.673646</td><td>-87.577965</td></tr><tr><td>10</td><td>2024-01-01</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>41.692895</td><td>-87.926803</td></tr><tr><td>20</td><td>2024-01-01</td><td>2</td><td>0</td><td>1</td><td>0</td><td>0</td><td>41.735961</td><td>-87.937182</td></tr><tr><td>21</td><td>2024-01-01</td><td>2</td><td>1</td><td>1</td><td>0</td><td>0</td><td>41.722849</td><td>-87.896262</td></tr><tr><td>24</td><td>2024-01-01</td><td>2</td><td>4</td><td>1</td><td>0</td><td>0</td><td>41.72837</td><td>-87.748934</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 9)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ cell_id â”† date       â”† lat_bin â”† lon_bin â”† â€¦ â”† arrest_cou â”† domestic_c â”† centroid_l â”† centroid_l â”‚\n",
       "â”‚ ---     â”† ---        â”† ---     â”† ---     â”†   â”† nt         â”† ount       â”† at         â”† on         â”‚\n",
       "â”‚ i32     â”† date       â”† i32     â”† i32     â”†   â”† ---        â”† ---        â”† ---        â”† ---        â”‚\n",
       "â”‚         â”†            â”†         â”†         â”†   â”† u32        â”† u32        â”† f64        â”† f64        â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 8       â”† 2024-01-01 â”† 0       â”† 8       â”† â€¦ â”† 1          â”† 0          â”† 41.673646  â”† -87.577965 â”‚\n",
       "â”‚ 10      â”† 2024-01-01 â”† 1       â”† 0       â”† â€¦ â”† 0          â”† 0          â”† 41.692895  â”† -87.926803 â”‚\n",
       "â”‚ 20      â”† 2024-01-01 â”† 2       â”† 0       â”† â€¦ â”† 0          â”† 0          â”† 41.735961  â”† -87.937182 â”‚\n",
       "â”‚ 21      â”† 2024-01-01 â”† 2       â”† 1       â”† â€¦ â”† 0          â”† 0          â”† 41.722849  â”† -87.896262 â”‚\n",
       "â”‚ 24      â”† 2024-01-01 â”† 2       â”† 4       â”† â€¦ â”† 0          â”† 0          â”† 41.72837   â”† -87.748934 â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define grid dimensions\n",
    "n_lat_bins, n_lon_bins = 10, 10\n",
    "print(f\"ğŸ—ºï¸  Creating a {n_lat_bins}Ã—{n_lon_bins} spatial grid ({n_lat_bins * n_lon_bins} cells total)\")\n",
    "\n",
    "# Add spatial bin columns and temporal features\n",
    "crime_df = crime_df.with_columns([\n",
    "    # Compute which grid cell each crime falls into\n",
    "    ((pl.col(\"latitude\") - lat_min) / (lat_max - lat_min) * n_lat_bins).floor().cast(pl.Int32).alias(\"lat_bin\"),\n",
    "    ((pl.col(\"longitude\") - lon_min) / (lon_max - lon_min) * n_lon_bins).floor().cast(pl.Int32).alias(\"lon_bin\"),\n",
    "    # Extract date (for daily aggregation)\n",
    "    pl.col(\"timestamp\").dt.date().alias(\"date\"),\n",
    "    # Extract hour (useful for temporal patterns)\n",
    "    pl.col(\"timestamp\").dt.hour().alias(\"hour\"),\n",
    "])\n",
    "\n",
    "# Create unique cell ID from lat/lon bins\n",
    "crime_df = crime_df.with_columns(\n",
    "    (pl.col(\"lat_bin\") * n_lon_bins + pl.col(\"lon_bin\")).alias(\"cell_id\")\n",
    ")\n",
    "\n",
    "# Aggregate to daily counts per cell\n",
    "daily_counts = crime_df.group_by([\"cell_id\", \"date\", \"lat_bin\", \"lon_bin\"]).agg([\n",
    "    pl.len().alias(\"event_count\"),           # Total crimes in this cell-day\n",
    "    pl.col(\"arrest\").sum().alias(\"arrest_count\"),     # How many led to arrests\n",
    "    pl.col(\"domestic\").sum().alias(\"domestic_count\"), # Domestic incidents\n",
    "    pl.col(\"latitude\").mean().alias(\"centroid_lat\"),  # Cell centroid\n",
    "    pl.col(\"longitude\").mean().alias(\"centroid_lon\"),\n",
    "]).sort([\"date\", \"cell_id\"])\n",
    "\n",
    "print(f\"\\nğŸ“Š Aggregation Results:\")\n",
    "print(f\"   â€¢ Original records: {len(crime_df):,}\")\n",
    "print(f\"   â€¢ Aggregated rows: {len(daily_counts):,} (cell Ã— day combinations)\")\n",
    "print(f\"   â€¢ Unique cells with events: {daily_counts['cell_id'].n_unique()}\")\n",
    "print(f\"   â€¢ Date range: {daily_counts['date'].min()} to {daily_counts['date'].max()}\")\n",
    "print(f\"\\nğŸ” Sample aggregated data:\")\n",
    "daily_counts.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d896dea",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“ˆ Adapter 1: TableAdapter (For Regression Models)\n",
    "\n",
    "### When to Use\n",
    "Use `TableAdapter` when you want to predict **counts** or **rates** using:\n",
    "- **Poisson/Negative Binomial GLM** - Classic count regression\n",
    "- **XGBoost/LightGBM** - Gradient boosting\n",
    "- **Random Forest** - Ensemble methods\n",
    "- **Linear models** - Ridge, Lasso\n",
    "\n",
    "### Key Features\n",
    "- **Offset column** - For exposure adjustment (e.g., population at risk)\n",
    "- **Weight column** - For observation weighting\n",
    "- **Intercept** - Optional intercept column for GLMs\n",
    "- **get_X_y()** - Returns numpy arrays ready for sklearn\n",
    "\n",
    "### Configuration Options\n",
    "```python\n",
    "TableAdapterConfig(\n",
    "    target_col=\"event_count\",     # Column to predict\n",
    "    feature_cols=[...],            # Features to include (auto-detected if None)\n",
    "    offset_col=\"exposure\",         # Log-offset for Poisson GLM\n",
    "    weight_col=\"weight\",           # Observation weights\n",
    "    include_intercept=True,        # Add column of 1s\n",
    "    categorical_encoding=\"onehot\", # How to handle categoricals\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dc44d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 03:09:46,284 - eventflow.core.adapters.table - INFO - Converting EventFrame to table format\n",
      "2025-12-01 03:09:46,286 - eventflow.core.adapters.table - INFO - Created table with 858 rows, 6 features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š TableAdapter Output:\n",
      "   â€¢ Data shape: (858, 8)\n",
      "   â€¢ Feature names: ['_intercept', 'lat_bin', 'lon_bin', 'day_of_week', 'arrest_count', 'domestic_count']\n",
      "   â€¢ Target column: 'event_count'\n",
      "   â€¢ Offset column: 'exposure'\n",
      "\n",
      "ğŸ”¢ NumPy Arrays (ready for sklearn):\n",
      "   â€¢ X shape: (858, 6) (samples Ã— features)\n",
      "   â€¢ y shape: (858,) (samples,)\n",
      "   â€¢ X dtype: float32\n",
      "\n",
      "ğŸ“‹ First 3 rows of X:\n",
      "[[1. 0. 8. 1. 1. 0.]\n",
      " [1. 1. 0. 1. 0. 0.]\n",
      " [1. 2. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for TableAdapter\n",
    "# Add day-of-week feature and uniform exposure\n",
    "table_df = daily_counts.with_columns([\n",
    "    pl.col(\"date\").dt.weekday().alias(\"day_of_week\"),  # 0=Monday, 6=Sunday\n",
    "    pl.lit(1.0).alias(\"exposure\"),  # Uniform exposure (could be population)\n",
    "])\n",
    "\n",
    "# Configure the adapter\n",
    "table_config = TableAdapterConfig(\n",
    "    target_col=\"event_count\",  # What we want to predict\n",
    "    feature_cols=[\"lat_bin\", \"lon_bin\", \"day_of_week\", \"arrest_count\", \"domestic_count\"],\n",
    "    offset_col=\"exposure\",     # For Poisson regression: log(exposure) offset\n",
    "    include_intercept=True,    # Add intercept column for GLM\n",
    ")\n",
    "\n",
    "# Convert!\n",
    "table_adapter = TableAdapter(table_config)\n",
    "table_output = table_adapter.convert(table_df)\n",
    "\n",
    "print(\"ğŸ“Š TableAdapter Output:\")\n",
    "print(f\"   â€¢ Data shape: {table_output.data.shape}\")\n",
    "print(f\"   â€¢ Feature names: {table_output.feature_names}\")\n",
    "print(f\"   â€¢ Target column: '{table_output.target}'\")\n",
    "print(f\"   â€¢ Offset column: '{table_output.offset}'\")\n",
    "\n",
    "# Get sklearn-ready arrays\n",
    "X, y = table_output.get_X_y()\n",
    "print(f\"\\nğŸ”¢ NumPy Arrays (ready for sklearn):\")\n",
    "print(f\"   â€¢ X shape: {X.shape} (samples Ã— features)\")\n",
    "print(f\"   â€¢ y shape: {y.shape} (samples,)\")\n",
    "print(f\"   â€¢ X dtype: {X.dtype}\")\n",
    "print(f\"\\nğŸ“‹ First 3 rows of X:\")\n",
    "print(X[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b21400",
   "metadata": {},
   "source": [
    "### Example: Fitting a Poisson GLM\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import PoissonRegressor\n",
    "\n",
    "X, y = table_output.get_X_y()\n",
    "model = PoissonRegressor()\n",
    "model.fit(X, y)\n",
    "predictions = model.predict(X)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf081f2",
   "metadata": {},
   "source": [
    "## ğŸ”„ Adapter 2: SequenceAdapter (For RNN/Transformer)\n",
    "\n",
    "### When to Use\n",
    "Use `SequenceAdapter` when you want to model **temporal patterns** at each location:\n",
    "- **LSTM/GRU** - Recurrent neural networks\n",
    "- **Transformer** - Self-attention models\n",
    "- **Temporal Convolutional Networks** - 1D convolutions over time\n",
    "\n",
    "### Key Features\n",
    "- **Padding** - Handles variable-length sequences (left or right padding)\n",
    "- **Attention masks** - Indicates which positions are real vs. padded\n",
    "- **Time encoding** - Optional positional or sinusoidal embeddings\n",
    "\n",
    "### Output Shape\n",
    "```\n",
    "sequences: (n_locations, max_time_steps, n_features)\n",
    "masks:     (n_locations, max_time_steps)  # 1=real, 0=padded\n",
    "lengths:   (n_locations,)  # actual sequence length per location\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01898694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 03:09:53,173 - eventflow.core.adapters.sequence - INFO - Converting EventFrame to sequence format\n",
      "2025-12-01 03:09:53,198 - eventflow.core.adapters.sequence - INFO - Created 100 sequences with max_length=30, n_features=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Input: 858 rows covering 100 locations\n",
      "\n",
      "ğŸ”„ SequenceAdapter Output:\n",
      "   â€¢ Sequences shape: (100, 30, 1)\n",
      "     â†’ 100 locations Ã— 30 time steps Ã— 1 features\n",
      "   â€¢ Masks shape: (100, 30)\n",
      "   â€¢ Sequence lengths (first 5): [10  9  8  9 10]\n",
      "\n",
      "ğŸ“‹ Example: First location's sequence (first 10 time steps):\n",
      "   Values: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   Mask:   [ True  True  True  True  True  True  True  True  True  True]  (1=real data, 0=padding)\n"
     ]
    }
   ],
   "source": [
    "# Prepare sequence data\n",
    "sequence_data = daily_counts.select([\"cell_id\", \"date\", \"event_count\"]).sort([\"cell_id\", \"date\"])\n",
    "\n",
    "print(f\"ğŸ“Š Input: {len(sequence_data)} rows covering {sequence_data['cell_id'].n_unique()} locations\")\n",
    "\n",
    "# Configure the adapter\n",
    "seq_config = SequenceAdapterConfig(\n",
    "    spatial_col=\"cell_id\",       # Group sequences by this column\n",
    "    timestamp_col=\"date\",         # Order within each group\n",
    "    feature_cols=[\"event_count\"], # Features at each time step\n",
    "    sequence_length=30,           # Pad/truncate to this length\n",
    "    padding_value=0.0,            # Value for padded positions\n",
    "    padding_side=\"right\",         # Pad at the end (common for RNNs)\n",
    "    return_masks=True,            # Return attention masks\n",
    ")\n",
    "\n",
    "# Convert!\n",
    "seq_adapter = SequenceAdapter(seq_config)\n",
    "seq_output = seq_adapter.convert(sequence_data)\n",
    "\n",
    "print(f\"\\nğŸ”„ SequenceAdapter Output:\")\n",
    "print(f\"   â€¢ Sequences shape: {seq_output.sequences.shape}\")\n",
    "print(f\"     â†’ {seq_output.sequences.shape[0]} locations Ã— {seq_output.sequences.shape[1]} time steps Ã— {seq_output.sequences.shape[2]} features\")\n",
    "print(f\"   â€¢ Masks shape: {seq_output.masks.shape}\")\n",
    "print(f\"   â€¢ Sequence lengths (first 5): {seq_output.lengths[:5]}\")\n",
    "\n",
    "# Show an example sequence\n",
    "print(f\"\\nğŸ“‹ Example: First location's sequence (first 10 time steps):\")\n",
    "print(f\"   Values: {seq_output.sequences[0, :10, 0]}\")\n",
    "print(f\"   Mask:   {seq_output.masks[0, :10]}  (1=real data, 0=padding)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29be46e6",
   "metadata": {},
   "source": [
    "### Example: Using with PyTorch LSTM\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Convert to tensors\n",
    "sequences = torch.from_numpy(seq_output.sequences)  # (batch, seq_len, features)\n",
    "masks = torch.from_numpy(seq_output.masks)          # (batch, seq_len)\n",
    "\n",
    "# Create LSTM\n",
    "lstm = nn.LSTM(input_size=1, hidden_size=32, batch_first=True)\n",
    "output, (h_n, c_n) = lstm(sequences)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c1dbac",
   "metadata": {},
   "source": [
    "## ğŸ–¼ï¸ Adapter 3: RasterAdapter (For CNN Models)\n",
    "\n",
    "### When to Use\n",
    "Use `RasterAdapter` when you want to treat the city as an **image** and exploit spatial patterns:\n",
    "- **CNN** - 2D convolutions for spatial patterns\n",
    "- **ConvLSTM** - Spatio-temporal convolutions\n",
    "- **U-Net** - Encoder-decoder for dense prediction\n",
    "\n",
    "### Key Features\n",
    "- **Channel-first/last** - PyTorch (C,H,W) vs TensorFlow (H,W,C)\n",
    "- **Multiple channels** - Stack different features as channels\n",
    "- **Fill value** - Handle cells with no data\n",
    "- **Normalization** - Optional channel normalization\n",
    "\n",
    "### Output Shape\n",
    "```\n",
    "Channel-first (PyTorch): (n_timesteps, n_channels, height, width)\n",
    "Channel-last (TensorFlow): (n_timesteps, height, width, n_channels)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a236849",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 03:10:01,557 - eventflow.core.adapters.raster - INFO - Converting EventFrame to raster format\n",
      "2025-12-01 03:10:01,564 - eventflow.core.adapters.raster - INFO - Created raster with shape (30, 2, 10, 10): 30 timesteps, 2 channels, 10x10 grid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ–¼ï¸ RasterAdapter Output:\n",
      "   â€¢ Raster shape: (30, 2, 10, 10)\n",
      "     â†’ 30 time steps\n",
      "     â†’ 2 channels: ['event_count', 'arrest_count']\n",
      "     â†’ 10Ã—10 spatial grid\n",
      "   â€¢ Timestamps: 30 dates\n",
      "\n",
      "ğŸ“‹ First day, 'event_count' channel (10Ã—10 grid):\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 1. 1. 0. 0. 1. 0. 0. 1.]\n",
      " [1. 0. 0. 1. 0. 0. 1. 0. 0. 1.]\n",
      " [2. 1. 0. 1. 1. 0. 0. 0. 2. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 2. 0. 1. 0. 0.]\n",
      " [0. 2. 2. 1. 0. 0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Configure the adapter\n",
    "raster_config = RasterAdapterConfig(\n",
    "    grid_col=\"cell_id\",          # Maps to grid position\n",
    "    timestamp_col=\"date\",         # Creates time dimension\n",
    "    feature_cols=[\"event_count\", \"arrest_count\"],  # Each becomes a channel\n",
    "    grid_shape=(n_lat_bins, n_lon_bins),  # 10Ã—10 grid\n",
    "    channel_first=True,           # PyTorch format (C, H, W)\n",
    "    fill_value=0.0,               # Empty cells get 0\n",
    "    normalize=False,              # Keep raw counts\n",
    ")\n",
    "\n",
    "# Convert!\n",
    "raster_adapter = RasterAdapter(raster_config)\n",
    "raster_output = raster_adapter.convert(daily_counts)\n",
    "\n",
    "print(f\"ğŸ–¼ï¸ RasterAdapter Output:\")\n",
    "print(f\"   â€¢ Raster shape: {raster_output.raster.shape}\")\n",
    "print(f\"     â†’ {raster_output.raster.shape[0]} time steps\")\n",
    "print(f\"     â†’ {raster_output.raster.shape[1]} channels: {raster_output.channel_names}\")\n",
    "print(f\"     â†’ {raster_output.raster.shape[2]}Ã—{raster_output.raster.shape[3]} spatial grid\")\n",
    "print(f\"   â€¢ Timestamps: {len(raster_output.timestamps)} dates\")\n",
    "\n",
    "# Visualize one time slice\n",
    "print(f\"\\nğŸ“‹ First day, 'event_count' channel (10Ã—10 grid):\")\n",
    "print(raster_output.raster[0, 0])  # First timestep, first channel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7e988e",
   "metadata": {},
   "source": [
    "### Example: Using with PyTorch CNN\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Shape: (batch, channels, height, width)\n",
    "x = torch.from_numpy(raster_output.raster)  # (30, 2, 10, 10)\n",
    "\n",
    "# Simple CNN for crime prediction\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(2, 16, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 1, kernel_size=3, padding=1),  # Predict next day\n",
    ")\n",
    "predictions = model(x)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b967a4",
   "metadata": {},
   "source": [
    "## ğŸ•¸ï¸ Adapter 4: GraphAdapter (For GNN Models)\n",
    "\n",
    "### When to Use\n",
    "Use `GraphAdapter` when you want to model **relationships between locations**:\n",
    "- **GCN** - Graph Convolutional Networks\n",
    "- **GAT** - Graph Attention Networks\n",
    "- **GraphSAGE** - Inductive learning on graphs\n",
    "\n",
    "### Key Features\n",
    "- **Adjacency types** - Spatial proximity, temporal, or both\n",
    "- **Edge features** - Optional features on edges\n",
    "- **Self-loops** - Include self-connections\n",
    "- **Normalization** - Symmetric or row normalization\n",
    "\n",
    "### Output Components\n",
    "```python\n",
    "node_features: (n_nodes, n_features)  # Feature vector per node\n",
    "edge_index: (2, n_edges)              # COO format edges [src, dst]\n",
    "adjacency: (n_nodes, n_nodes)         # Dense adjacency matrix\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb45b22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 03:10:13,943 - eventflow.core.adapters.graph - INFO - Converting EventFrame to graph format\n",
      "2025-12-01 03:10:13,962 - eventflow.core.adapters.graph - INFO - Created graph with 100 nodes, 476 edges, 5 features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Node data: 100 cells (nodes)\n",
      "\n",
      "ğŸ•¸ï¸ GraphAdapter Output:\n",
      "   â€¢ Node features: (100, 5)\n",
      "     â†’ 100 nodes Ã— 5 features\n",
      "   â€¢ Edge index: (2, 476)\n",
      "     â†’ 476 edges total\n",
      "   â€¢ Adjacency matrix: (100, 100)\n",
      "\n",
      "ğŸ“‹ First 5 edges (node â†’ node):\n",
      "   0 â†’ 1\n",
      "   1 â†’ 0\n",
      "   0 â†’ 10\n",
      "   10 â†’ 0\n",
      "   1 â†’ 2\n"
     ]
    }
   ],
   "source": [
    "# Aggregate to get one feature vector per cell (node)\n",
    "node_features_df = daily_counts.group_by(\"cell_id\").agg([\n",
    "    pl.col(\"event_count\").sum().alias(\"total_events\"),\n",
    "    pl.col(\"event_count\").mean().alias(\"avg_events\"),\n",
    "    pl.col(\"arrest_count\").sum().alias(\"total_arrests\"),\n",
    "    pl.col(\"centroid_lat\").mean().alias(\"lat\"),\n",
    "    pl.col(\"centroid_lon\").mean().alias(\"lon\"),\n",
    "]).sort(\"cell_id\")\n",
    "\n",
    "print(f\"ğŸ“Š Node data: {len(node_features_df)} cells (nodes)\")\n",
    "\n",
    "# Configure the adapter\n",
    "graph_config = GraphAdapterConfig(\n",
    "    node_col=\"cell_id\",           # Node identifier\n",
    "    feature_cols=[\"total_events\", \"avg_events\", \"total_arrests\", \"lat\", \"lon\"],\n",
    "    adjacency_type=\"spatial\",     # Connect nearby nodes\n",
    "    spatial_threshold=0.05,        # ~5km radius for edges\n",
    "    include_self_loops=True,       # Nodes connect to themselves\n",
    "    normalize_adjacency=True,      # Normalize for GCN\n",
    ")\n",
    "\n",
    "# Convert!\n",
    "graph_adapter = GraphAdapter(graph_config)\n",
    "graph_output = graph_adapter.convert(node_features_df)\n",
    "\n",
    "print(f\"\\nğŸ•¸ï¸ GraphAdapter Output:\")\n",
    "print(f\"   â€¢ Node features: {graph_output.node_features.shape}\")\n",
    "print(f\"     â†’ {graph_output.node_features.shape[0]} nodes Ã— {graph_output.node_features.shape[1]} features\")\n",
    "print(f\"   â€¢ Edge index: {graph_output.edge_index.shape}\")\n",
    "print(f\"     â†’ {graph_output.edge_index.shape[1]} edges total\")\n",
    "print(f\"   â€¢ Adjacency matrix: {graph_output.adjacency.shape}\")\n",
    "\n",
    "# Show sample edges\n",
    "print(f\"\\nğŸ“‹ First 5 edges (node â†’ node):\")\n",
    "for i in range(min(5, graph_output.edge_index.shape[1])):\n",
    "    src, dst = graph_output.edge_index[0, i], graph_output.edge_index[1, i]\n",
    "    print(f\"   {src} â†’ {dst}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819c0d22",
   "metadata": {},
   "source": [
    "### Example: Using with PyTorch Geometric\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# Create PyG Data object\n",
    "data = Data(\n",
    "    x=torch.from_numpy(graph_output.node_features),\n",
    "    edge_index=torch.from_numpy(graph_output.edge_index),\n",
    ")\n",
    "\n",
    "# Simple GCN layer\n",
    "conv = GCNConv(in_channels=5, out_channels=16)\n",
    "node_embeddings = conv(data.x, data.edge_index)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bd8ef1",
   "metadata": {},
   "source": [
    "## â±ï¸ Adapter 5: StreamAdapter (For Neural ODE / Point Processes)\n",
    "\n",
    "### When to Use\n",
    "Use `StreamAdapter` when you want to model **continuous-time dynamics**:\n",
    "- **Neural ODE** - Continuous-depth networks\n",
    "- **Temporal Point Processes** - Hawkes, self-exciting models\n",
    "- **Continuous-time RNNs** - ODE-RNN, GRU-ODE\n",
    "\n",
    "### Key Features\n",
    "- **Inter-event times** - Time between consecutive events\n",
    "- **Event types (marks)** - Categorical event labels\n",
    "- **Time scaling** - Normalize, log-transform, or raw\n",
    "- **Relative timestamps** - Origin at first event or custom\n",
    "\n",
    "### Output Components\n",
    "```python\n",
    "timestamps: (n_events,)    # Scaled event times\n",
    "inter_times: (n_events,)   # Time since previous event\n",
    "states: (n_events, n_features)  # State at each event\n",
    "event_types: (n_events,)   # Encoded event type\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "974ab99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 03:10:34,400 - eventflow.core.adapters.stream - INFO - Converting EventFrame to stream format\n",
      "2025-12-01 03:10:34,403 - eventflow.core.adapters.stream - INFO - Created stream with 1000 events, 2 state dims\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Stream data: 1000 individual events\n",
      "\n",
      "â±ï¸ StreamAdapter Output:\n",
      "   â€¢ Timestamps: (1000,) (normalized)\n",
      "   â€¢ States: (1000, 2) (lat, lon per event)\n",
      "   â€¢ Inter-event times: (1000,)\n",
      "   â€¢ Event types: (1000,)\n",
      "   â€¢ Unique event types: [0 1 2 3 4 5]\n",
      "\n",
      "ğŸ“‹ First 5 events:\n",
      "   Timestamps (normalized): [-1.6646022 -1.6646022 -1.6646022 -1.6599252 -1.6599252]\n",
      "   Inter-event times: [0.         0.         0.         0.00467694 0.        ]\n",
      "   Event types: [3 1 2 4 1]\n"
     ]
    }
   ],
   "source": [
    "# Use individual events (not aggregated) for continuous-time modeling\n",
    "stream_df = crime_df.select([\n",
    "    \"case_id\", \"timestamp\", \"primary_type\", \"latitude\", \"longitude\"\n",
    "]).sort(\"timestamp\")\n",
    "\n",
    "print(f\"ğŸ“Š Stream data: {len(stream_df)} individual events\")\n",
    "\n",
    "# Configure the adapter\n",
    "stream_config = StreamAdapterConfig(\n",
    "    timestamp_col=\"timestamp\",     # Event times\n",
    "    event_type_col=\"primary_type\", # Event marks (crime type)\n",
    "    state_cols=[\"latitude\", \"longitude\"],  # State vector\n",
    "    time_scale=\"normalize\",        # Normalize to mean=0, std=1\n",
    "    time_origin=\"first\",           # Start from first event\n",
    "    return_inter_times=True,       # Compute inter-event times\n",
    ")\n",
    "\n",
    "# Convert!\n",
    "stream_adapter = StreamAdapter(stream_config)\n",
    "stream_output = stream_adapter.convert(stream_df)\n",
    "\n",
    "print(f\"\\nâ±ï¸ StreamAdapter Output:\")\n",
    "print(f\"   â€¢ Timestamps: {stream_output.timestamps.shape} (normalized)\")\n",
    "print(f\"   â€¢ States: {stream_output.states.shape} (lat, lon per event)\")\n",
    "print(f\"   â€¢ Inter-event times: {stream_output.inter_times.shape}\")\n",
    "print(f\"   â€¢ Event types: {stream_output.event_types.shape}\")\n",
    "print(f\"   â€¢ Unique event types: {np.unique(stream_output.event_types)}\")\n",
    "\n",
    "# Show sample\n",
    "print(f\"\\nğŸ“‹ First 5 events:\")\n",
    "print(f\"   Timestamps (normalized): {stream_output.timestamps[:5]}\")\n",
    "print(f\"   Inter-event times: {stream_output.inter_times[:5]}\")\n",
    "print(f\"   Event types: {stream_output.event_types[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eb61b7",
   "metadata": {},
   "source": [
    "### Example: Using for Temporal Point Process\n",
    "\n",
    "```python\n",
    "# Inter-event times are key for TPP models\n",
    "inter_times = stream_output.inter_times  # Time gaps\n",
    "event_types = stream_output.event_types  # Crime types as integers\n",
    "\n",
    "# Hawkes process likelihood uses these directly\n",
    "# Neural TPP models (like THP, SAHP) use them as input sequences\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be01a9d5",
   "metadata": {},
   "source": [
    "## ğŸ“Š Summary: Choosing the Right Adapter\n",
    "\n",
    "| If you want to... | Use this adapter | Output format |\n",
    "|-------------------|------------------|---------------|\n",
    "| Predict counts with features | `TableAdapter` | (samples, features) + (samples,) |\n",
    "| Model temporal patterns per location | `SequenceAdapter` | (locations, time, features) |\n",
    "| Treat the city as an image | `RasterAdapter` | (time, channels, H, W) |\n",
    "| Model spatial relationships | `GraphAdapter` | node_features + edge_index |\n",
    "| Model continuous-time events | `StreamAdapter` | timestamps + inter_times |\n",
    "\n",
    "### Serialization\n",
    "All adapters support saving to disk in multiple formats:\n",
    "```python\n",
    "# Save as NumPy\n",
    "adapter.serialize(output, \"model_input.npz\", format=\"numpy\")\n",
    "\n",
    "# Save as PyTorch\n",
    "adapter.serialize(output, \"model_input.pt\", format=\"pytorch\")\n",
    "\n",
    "# Save as Parquet (for tabular data)\n",
    "table_adapter.serialize(table_output, \"data.parquet\", format=\"parquet\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ Next Steps\n",
    "\n",
    "1. **Try with real data** - Download actual Chicago crime data from the City Data Portal\n",
    "2. **Add more features** - Weather, holidays, demographics\n",
    "3. **Build models** - Use the outputs with your favorite ML framework\n",
    "4. **Tune adapters** - Adjust grid resolution, time windows, graph connectivity\n",
    "\n",
    "For more information, see the EventFlow documentation and API reference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
