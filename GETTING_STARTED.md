# Eventflow Project - Getting Started

Welcome to **Eventflow**, a high-performance, generic spatio-temporal event transformation engine!

## Project Structure Created

The complete Eventflow project has been set up with the following structure:

```
eventflow/
â”œâ”€â”€ pyproject.toml           # Project configuration and dependencies
â”œâ”€â”€ README.md                # Main documentation
â”œâ”€â”€ LICENSE                  # MIT License
â”œâ”€â”€ .gitignore              # Git ignore patterns
â”œâ”€â”€ .pre-commit-config.yaml # Pre-commit hooks configuration
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml          # GitHub Actions CI workflow
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ datasets/
â”‚   â”‚   â””â”€â”€ chicago_crime_example.yaml
â”‚   â””â”€â”€ recipes/
â”‚       â””â”€â”€ chicago_crime_v1.yaml
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.md      # Architecture documentation
â”‚   â”œâ”€â”€ api_reference.md     # API reference
â”‚   â””â”€â”€ datasets.md          # Dataset documentation
â”œâ”€â”€ src/eventflow/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ core/               # Generic engine (dataset-agnostic)
â”‚   â”‚   â”œâ”€â”€ schema.py       # Schema definitions
â”‚   â”‚   â”œâ”€â”€ event_frame.py  # EventFrame abstraction
â”‚   â”‚   â”œâ”€â”€ spatial.py      # Spatial operations
â”‚   â”‚   â”œâ”€â”€ temporal.py     # Temporal operations
â”‚   â”‚   â”œâ”€â”€ features.py     # Feature engineering
â”‚   â”‚   â”œâ”€â”€ pipeline.py     # Pipeline orchestration
â”‚   â”‚   â”œâ”€â”€ utils.py        # Utilities
â”‚   â”‚   â””â”€â”€ context/        # Context enrichment
â”‚   â”‚       â”œâ”€â”€ sources.py
â”‚   â”‚       â”œâ”€â”€ joiners.py
â”‚   â”‚       â””â”€â”€ enricher.py
â”‚   â”œâ”€â”€ datasets/           # Dataset adapters
â”‚   â”‚   â””â”€â”€ chicago_crime/
â”‚   â”‚       â”œâ”€â”€ schema.py
â”‚   â”‚       â”œâ”€â”€ mapping.py
â”‚   â”‚       â”œâ”€â”€ context/
â”‚   â”‚       â”‚   â”œâ”€â”€ weather.py
â”‚   â”‚       â”‚   â”œâ”€â”€ events.py
â”‚   â”‚       â”‚   â””â”€â”€ demographics.py
â”‚   â”‚       â””â”€â”€ recipes/
â”‚   â”‚           â””â”€â”€ chicago_crime_v1.py
â”‚   â”œâ”€â”€ recipes/            # Recipe mechanism
â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â””â”€â”€ registry.py
â”‚   â”œâ”€â”€ tracking/           # Experiment tracking
â”‚   â”‚   â”œâ”€â”€ protocol.py
â”‚   â”‚   â””â”€â”€ mlflow_tracker.py
â”‚   â””â”€â”€ cli/                # Command-line interface
â”‚       â””â”€â”€ main.py
â”œâ”€â”€ tests/                  # Unit and integration tests
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ test_schema.py
â”‚   â”‚   â””â”€â”€ test_event_frame.py
â”‚   â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ recipes/
â”‚   â”‚   â””â”€â”€ test_registry.py
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ conftest.py
â””â”€â”€ scripts/                # Example scripts
    â”œâ”€â”€ run_chicago_recipe.py
    â””â”€â”€ run_experiment.py
```

## Next Steps

### 1. Install Dependencies

Navigate to the project directory and install in development mode:

```powershell
cd eventflow
pip install -e ".[dev]"
```

This will install:
- Core dependencies: polars, pydantic, shapely, pyproj, typer, pyyaml
- Development tools: pytest, black, ruff, mypy, pre-commit
- Optional: Install MLflow with `pip install eventflow[tracking]`

### 2. Set Up Pre-commit Hooks

```powershell
pre-commit install
```

This configures automatic code formatting and linting on commit.

### 3. Run Tests

```powershell
pytest
```

Or with coverage:

```powershell
pytest --cov=eventflow --cov-report=html
```

### 4. Try the CLI

```powershell
# List available datasets
python -m eventflow.cli.main list-datasets

# List recipes
python -m eventflow.cli.main list-recipes

# Validate a config
python -m eventflow.cli.main validate --config configs/recipes/chicago_crime_v1.yaml

# Check version
python -m eventflow.cli.main version
```

### 5. Run Example Scripts

```powershell
# Run Chicago Crime recipe (requires data)
python scripts/run_chicago_recipe.py

# Run experiment with tracking
python scripts/run_experiment.py
```

## Key Features Implemented

### Core Module
- **EventFrame**: Central abstraction wrapping Polars LazyFrame with schema and metadata
- **Schema Definitions**: Pydantic models for events and context sources
- **Spatial Operations**: Grid construction, coordinate transformation, zone assignment
- **Temporal Operations**: Time binning, component extraction, temporal alignment
- **Feature Engineering**: Aggregations, moving windows, categorical encoding
- **Pipeline**: Composable step-based transformations
- **Context Enrichment**: Generic framework for joining external data

### Dataset Module
- **Chicago Crime Adapter**: Complete implementation with schema, mapping, and loaders
- **Context Sources**: Weather, special events, demographics sources
- **Recipes**: Pre-built feature engineering pipelines

### Recipe Module
- **Base Recipe Interface**: Abstract base class for all recipes
- **Recipe Registry**: Discovery and instantiation system

### Tracking Module
- **Protocol-based Design**: Tracker protocol for any backend
- **MLflow Implementation**: Full MLflow integration

### CLI Module
- **Typer-based CLI**: User-friendly command-line interface
- Commands for running recipes, listing datasets, validation

## Development Workflow

### Adding a New Dataset

1. Create package: `src/eventflow/datasets/your_dataset/`
2. Define schema in `schema.py`
3. Implement loader in `mapping.py`
4. Add context sources in `context/`
5. Create recipes in `recipes/`
6. Add tests in `tests/datasets/your_dataset/`
7. Document in `docs/datasets.md`

### Adding a New Recipe

1. Create recipe class extending `BaseRecipe`
2. Implement `build_pipeline()` method
3. Register with `register_recipe()`
4. Add configuration YAML in `configs/recipes/`
5. Add tests
6. Document usage

### Code Quality

The project uses:
- **Black** for code formatting
- **Ruff** for linting
- **MyPy** for type checking
- **Pytest** for testing

Run all checks:

```powershell
black src tests
ruff check src tests
mypy src
pytest
```

## Architecture Principles

1. **Lazy Evaluation**: All operations use Polars LazyFrame until `.collect()`
2. **Generic Core**: No dataset-specific logic in `core/`
3. **Type Safety**: Extensive use of Pydantic for validation
4. **Composability**: Pure functions and stateless steps
5. **Extensibility**: Protocol-based design for tracking, context sources

## Important Notes

- **Import Errors**: You'll see import errors until dependencies are installed with `pip install -e ".[dev]"`
- **Data Not Included**: Raw data files are not part of the repository - configure paths in YAML files
- **Production Ready**: Core abstractions are production-ready; some implementations are placeholders for extension

## Resources

- **Documentation**: See `docs/` directory for architecture, API reference, and dataset guides
- **Examples**: Check `scripts/` for usage examples
- **Tests**: Review `tests/` for usage patterns and fixtures
- **Configs**: Examine `configs/` for configuration examples

## Contributing

1. Create a feature branch
2. Make changes following the architecture principles
3. Add tests for new functionality
4. Run pre-commit hooks and tests
5. Submit pull request

## Getting Help

- Read the architecture documentation: `docs/architecture.md`
- Check API reference: `docs/api_reference.md`
- Review example scripts in `scripts/`
- Look at test files for usage examples

## License

MIT License - see LICENSE file for details.

---

**You're all set!** Start by installing dependencies and running the tests. Happy coding! ðŸš€
